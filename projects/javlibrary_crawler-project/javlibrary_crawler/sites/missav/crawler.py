# -*- coding: utf-8 -*-

"""
"""

import typing as T
import os
import time
import json
import math
import gzip
from datetime import datetime, timedelta

from s3pathlib import S3Path, ContentTypeEnum
import aws_console_url.api as aws_console_url
import aws_arns.api as aws_arns
import pynamodb_mate.api as pm
import sqlalchemy.orm as orm
import sqlalchemy_mate.api as sam

from github import Github

from javlibrary_crawler.utils import (
    get_utc_now,
    to_s3_key_friendly_url,
    preview_export_details,
)
from javlibrary_crawler.logger import logger
from javlibrary_crawler.boto_ses import bsm
from javlibrary_crawler.runtime import runtime
from javlibrary_crawler.config.load import config
from javlibrary_crawler.utils import prompt_to_confirm
from javlibrary_crawler.vendor.dynamodb_export_to_s3 import Export
from javlibrary_crawler.vendor.waiter import Waiter
from javlibrary_crawler.vendor.hashes import hashes, HashAlgoEnum

from .sqlitedb import Base, Step2ParseHtmlStatusEnum, Job
from .constants import (
    LangCodeEnum,
    N_PENDING_SHARD,
    GITHUB_ACTION_RUN_INTERVAL,
    TASK_PROCESSING_TIME,
)
from .paths import dir_missav
from .sitemap import SiteMapSnapshot, parse_item_xml, ItemUrl
from .dynamodb import (
    StatusAndUpdateTimeIndex,
    BaseTask,
    lang_to_step1_mapping,
)
from .downloader import MalformedHtmlError


@logger.emoji_block(
    msg="Insert todo List to DynamoDB",
    emoji="ðŸ“¥",
)
def import_dynamodb_table(
    snapshot_id: str,
    lang_code: LangCodeEnum,
    _first_k_file: T.Optional[int] = None,
    _first_k_url: T.Optional[int] = None,
):
    """ """
    sitemap_snapshot = SiteMapSnapshot.new(md5=snapshot_id)
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"Working on table {klass.Meta.table_name!r}")

    logger.info("WARNING: Import dynamodb table will the delete existing table!")
    prompt_to_confirm()
    if klass.exists():
        klass.delete_table()
        logger.info("wait 15 seconds for table to be deleted ...")
        time.sleep(15)

    path_list = sitemap_snapshot.get_item_xml_list()
    if _first_k_file:  # pragma: no cover
        path_list = path_list[:_first_k_file]
    with logger.indent():
        logger.info(f"Got {len(path_list)} xml file to insert")
    logger.info("")

    path_temp_json = dir_missav.joinpath("import_dynamodb_table_temp.json")
    path_temp_json_gzip = dir_missav.joinpath("import_dynamodb_table_temp.json.gz")
    path_temp_json.remove_if_exists()
    logger.info("Write import dynamodb table data to temp json file")
    with logger.indent():
        logger.info(f"preview at: file://{path_temp_json}")
    logger.info("")
    start_time = get_utc_now()
    i = 0
    with path_temp_json.open("a") as f:
        for path in path_list:
            logger.info(f"Working on {path.basename} file")
            item_url_list = parse_item_xml(path)
            filtered_item_url_list = ItemUrl.filter_by_lang(
                item_url_list=item_url_list,
                lang_code=lang_code,
            )
            if _first_k_url:  # pragma: no cover
                filtered_item_url_list = filtered_item_url_list[:_first_k_url]
            with logger.indent():
                logger.info(f"Got {len(filtered_item_url_list)} url to insert")

            for item in filtered_item_url_list:
                i += 1
                create_time = start_time + timedelta(microseconds=i)
                task = klass.make(
                    task_id=item.url, create_time=create_time, update_time=create_time
                )
                f.write(json.dumps({"Item": task.serialize()}) + "\n")
    logger.info(f"Got {i} DynamoDB items in total to import.")
    logger.info("")

    path_temp_json_gzip.write_bytes(gzip.compress(path_temp_json.read_bytes()))

    s3path_temp_json_gzip = config.env.s3path_missav_import_dynamodb_table_temp_json_gz
    logger.info("Upload temp json file to S3")
    with logger.indent():
        logger.info(f"preview at: {s3path_temp_json_gzip.console_url}")
    s3path_temp_json_gzip.upload_file(
        path=str(path_temp_json_gzip),
        overwrite=True,
        bsm=bsm,
        extra_args=dict(
            ContentType=ContentTypeEnum.app_gzip,
            Metadata={
                "sitemap_snapshot_id": snapshot_id,
                "lang_code": lang_code.name,
            },
        ),
    )

    md5 = hashes.of_file(
        str(path_temp_json_gzip),
        algo=HashAlgoEnum.md5,
        hexdigest=True,
    )
    now = get_utc_now()
    client_token = str(now.timestamp() // 300) + "-" + md5
    res = bsm.dynamodb_client.import_table(
        ClientToken=client_token,
        S3BucketSource=dict(
            S3Bucket=s3path_temp_json_gzip.bucket,
            S3KeyPrefix=s3path_temp_json_gzip.key,
        ),
        InputFormat="DYNAMODB_JSON",
        InputCompressionType="GZIP",
        TableCreationParameters=dict(
            TableName=klass.Meta.table_name,
            AttributeDefinitions=[
                dict(AttributeName=BaseTask.key.attr_name, AttributeType="S"),
                dict(AttributeName=BaseTask.value.attr_name, AttributeType="S"),
                dict(AttributeName=BaseTask.update_time.attr_name, AttributeType="S"),
            ],
            KeySchema=[
                dict(AttributeName=BaseTask.key.attr_name, KeyType="HASH"),
            ],
            BillingMode="PAY_PER_REQUEST",
            GlobalSecondaryIndexes=[
                dict(
                    IndexName=StatusAndUpdateTimeIndex.Meta.index_name,
                    KeySchema=[
                        dict(
                            AttributeName=StatusAndUpdateTimeIndex.value.attr_name,
                            KeyType="HASH",
                        ),
                        dict(
                            AttributeName=StatusAndUpdateTimeIndex.update_time.attr_name,
                            KeyType="RANGE",
                        ),
                    ],
                    Projection=dict(
                        ProjectionType="INCLUDE",
                        NonKeyAttributes=[
                            BaseTask.create_time.attr_name,
                        ],
                    ),
                )
            ],
        ),
    )
    import_arn = res["ImportTableDescription"]["ImportArn"]
    with logger.indent():
        logger.info(f"import_arn = {import_arn}")
        logger.info("be patient, it will take a while to import the table.")


@logger.emoji_block(
    msg="Insert todo List to DynamoDB",
    emoji="ðŸ“¥",
)
def insert_todo_list(
    snapshot_id: str,
    lang_code: LangCodeEnum,
    export_arn: T.Optional[str] = None,
):
    """
    **åŠŸèƒ½**

    æˆ‘ä»¬ä¼šå®šæœŸä»Ž sitemap.xml ä¸­ä¸‹è½½ç½‘ç«™ä¸Šæ‰€æœ‰çš„ URL çš„åˆ—è¡¨, ç„¶åŽå°†å…¶ä½œä¸ºå¾…çˆ¬ URL æ’å…¥åˆ°
    DynamoDB ä¸­.

    **æ’å…¥é¡ºåº**

    åœ¨æ’å…¥çš„æ—¶å€™, ä¼˜å…ˆæ’å…¥æ•°å­—å°çš„ sitemap_items_*.xml.gz æ–‡ä»¶ (æ¯”è¾ƒæ—§çš„æ–‡ä»¶). è¿™æ ·èƒ½ä¿è¯
    æ¯”è¾ƒæ–°çš„ url ä¼šæœ‰æ¯”è¾ƒæ–°çš„ update time, è¿™æ ·åœ¨ query çš„æ—¶å€™ç”¨ older_task_first = False
    å¯ä»¥ä¼˜å…ˆç­›é€‰å‡ºæ¯”è¾ƒæ–°çš„ url. ç”±äºŽæˆ‘ä»¬æ¯æ¬¡æ›´æ–°äº† sitemap.xml ä¹‹åŽéƒ½ä¼šèŽ·å¾—æ–°çš„ URL, æˆ‘ä»¬
    ä¹Ÿå¸Œæœ›ä¼˜å…ˆä¸‹è½½æ–°çš„ URL, æ‰€ä»¥è¿™ä¸ªæ’å…¥é¡ºåºåˆšå¥½èƒ½æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚.

    **Sitemap æ›´æ–°ç­–ç•¥**

    ç”±äºŽæ’å…¥æ–° URL æ˜¯ä¸€ä¸ªæ¯”è¾ƒ

    :param snapshot_id: sitemap çš„ MD5 å“ˆå¸Œå€¼
    :param lang_code: è¯­è¨€ä»£ç , è¿™ä¼šå†³å®šæ•°æ®ä¼šæ’å…¥åˆ°å“ªä¸ªè¡¨ä¸­
    :param export_arn: å¦‚æžœä½ çš„ DynamoDB ä¸­å·²ç»æœ‰å¾ˆå¤šæ•°æ®äº†, ç”±äºŽç”¨ DynamoDB ç›´æŽ¥
        filter å“ªäº› url å·²ç»å­˜åœ¨å“ªäº›ä¸å­˜åœ¨ä¸æ˜¯å¾ˆæ–¹ä¾¿, æ‰€ä»¥ä¼šæå‰å°†æ•°æ®å¯¼å‡ºåˆ° S3 ä¸­.
    """
    sitemap_snapshot = SiteMapSnapshot.new(md5=snapshot_id)
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")
    with klass.batch_write() as batch:
        # filter by only
        if export_arn:
            raise NotImplementedError
        else:
            path_list = sitemap_snapshot.get_item_xml_list()
            path_list = path_list[:1]
            logger.info(f"Got {len(path_list)} xml file to insert")
            for path in path_list:
                logger.info(f"Working on {path.basename} file")
                item_url_list = parse_item_xml(path)
                filtered_item_url_list = ItemUrl.filter_by_lang(
                    item_url_list, lang_code
                )
                filtered_item_url_list = filtered_item_url_list[:1000]
                with logger.indent():
                    logger.info(f"Got {len(filtered_item_url_list)} url to insert")
                for item in filtered_item_url_list:
                    task = klass.make_and_save(task_id=item.url)
                    batch.save(task)


@logger.emoji_block(
    msg="Crawl todo",
    emoji="ðŸ•¸",
)
def crawl_todo(
    lang_code: LangCodeEnum,
):
    """
    **åŠŸèƒ½**
    åŽ» DynamoDB ä¸­æ‰¾åˆ°æœªå®Œæˆçš„ä»»åŠ¡, å¹¶æ‰§è¡Œä¸‹è½½ä»»åŠ¡.

    **Crawler çš„è°ƒåº¦ç­–ç•¥**

    æˆ‘ä»¬åœ¨ ``javlibrary_crawler.sites.missav.constants.GITHUB_ACTION_RUN_INTERVAL`` ä¸­
    å®šä¹‰äº†æ¯ 15 åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ GitHub Action Workflow (ä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åº). æ³¨æ„è¿™é‡Œçš„ 15 åˆ†é’Ÿ
    åªæ˜¯ä¸€ä¸ªä¾‹å­, ç”¨äºŽè§£é‡Šæˆ‘ä»¬çš„è°ƒåº¦ç­–ç•¥. ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚æ¥è°ƒæ•´è¿™ä¸ªå€¼. åŽé¢çš„å¾ˆå¤šå…·ä½“æ•°å­—ä¹Ÿæ˜¯ç±»ä¼¼.

    ç„¶åŽæˆ‘ä»¬å®šä¹‰äº† ``javlibrary_crawler.sites.missav.constants.TASK_PROCESSING_TIME``
    ä¸º 2 ç§’ (æŠ“å– HTML, å†™å…¥ S3, æ›´æ–° DynamoDB åŠ çˆ¬è™«é—´éš”ç­‰å¾… 1 ç§’). äºŽæ˜¯æˆ‘ä»¬å¯ä»¥ç®—å‡ºæˆ‘ä»¬åœ¨
    900 - 30 (è¿™é‡Œçš„ 30 æ˜¯ä¸ºäº†ä¿é™©èµ·è§æå‰ 30 ç§’ç»“æŸ) = 870 ç§’å†…å¯ä»¥å¤„ç† 870 / 2 = 435 ä¸ªä»»åŠ¡.

    ç”±äºŽæˆ‘ä»¬çš„ status GSI ä½¿ç”¨äº† sharding ç­–ç•¥, è€Œåœ¨
    ``javlibrary_crawler.sites.missav.constants.N_PENDING_SHARD`` ä¸­å®šä¹‰äº†æˆ‘ä»¬æœ‰ 10 ä¸ª shard.

    æ‰€ä»¥æˆ‘ä»¬åœ¨ query_for_unfinished çš„æ—¶å€™è®¾å®šçš„ LIMIT ç­‰äºŽ 435 / 10 = 43.5, æˆ‘ä»¬å‘ä¸Šå–æ•´, å¾—åˆ° 44.
    è¿™æ ·å¯ä»¥å¤§çº¦æ¯æ¬¡è¿è¡Œéƒ½ä¼šå¤„ç† 435 ä¸ªä»»åŠ¡, é¿å…ä»Ž DynamoDB ä¸­è¯»å–è¿‡å¤šçš„æ•°æ®.
    """
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")

    # Cap the run time of this `crawl_todo` function.
    max_job_run_time = GITHUB_ACTION_RUN_INTERVAL - 30
    LIMIT = math.ceil(max_job_run_time / TASK_PROCESSING_TIME / N_PENDING_SHARD)
    # LIMIT = 1 # for debug only

    task_list: T.List[BaseTask] = klass.query_for_unfinished(
        limit=LIMIT,
        older_task_first=False,
    ).all()
    logger.info(f"Got {len(task_list)} URL to crawl.")

    if runtime.is_github_action:
        g = Github()
        github_run_id = int(os.environ["GITHUB_RUN_ID"])
        wf_run = g.get_repo("angoraking/javadb-project").get_workflow_run(github_run_id)
        start_at = wf_run.run_started_at
    else:
        start_at = get_utc_now()

    # the max time we can spend on each download task
    each_download_consumed_time = 5
    end_at = start_at + timedelta(seconds=max_job_run_time)
    for task in task_list:
        now = get_utc_now()
        logger.info(f"====== Working on {task.url} ======")
        logger.info(f"now is {now}, this job will end at {end_at}")
        if now >= end_at:
            logger.info("Time is up!")
            break

        how_many_time_left = (end_at - now).total_seconds()
        if how_many_time_left < each_download_consumed_time:
            logger.info("Time is up!")
            break

        try:
            with klass.start(
                task_id=task.task_id,
                debug=True,
            ) as exec_ctx:
                task_on_the_fly: BaseTask = exec_ctx.task
                task_on_the_fly.do_download_task()  # this function has auto retry
        except MalformedHtmlError as e:
            pass
        except Exception as e:
            raise e

        time.sleep(1)


def export_dynamodb(
    lang_code: LangCodeEnum,
    export_time: T.Optional[datetime] = None,
    delays: int = 10,
    timeout: int = 600,
    wait: bool = True,
) -> Export:
    """
    å°† DynamoDB æ•´ä¸ªè¡¨å¯¼å‡ºåˆ° S3 ä¸­.
    """
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")
    url = klass.get_table_overview_console_url()
    print(f"preview table: {url}")
    res = bsm.dynamodb_client.update_continuous_backups(
        TableName=klass.Meta.table_name,
        PointInTimeRecoverySpecification={"PointInTimeRecoveryEnabled": True},
    )

    if export_time is None:
        export_time = get_utc_now()
    client_token = export_time.strftime("%Y-%m-%d %H:%M:%S.%f")
    print(f"{client_token = }")
    table_arn = aws_arns.res.DynamodbTable.new(
        aws_account_id=bsm.aws_account_id,
        aws_region=bsm.aws_region,
        table_name=klass.Meta.table_name,
    ).to_arn()
    print(f"export s3 location = {config.env.s3dir_missav_dynamodb_exports.uri}")
    print(
        f"preview export s3 location = {config.env.s3dir_missav_dynamodb_exports.console_url}"
    )

    export = Export.export_table_to_point_in_time(
        dynamodb_client=bsm.dynamodb_client,
        table_arn=table_arn,
        s3_bucket=config.env.s3dir_missav_dynamodb_exports.bucket,
        s3_prefix=config.env.s3dir_missav_dynamodb_exports.key,
        export_time=export_time,
        client_token=client_token,
    )
    export_name = export.arn.split("/")[-1]
    aws_console = aws_console_url.AWSConsole.from_bsm(bsm)
    export_url = aws_console.dynamodb.get_table_export(
        table_or_arn=table_arn,
        export_name=export_name,
    )
    print(f"{export_url = }")

    if wait:
        for _ in Waiter(
            delays=delays,
            timeout=timeout,
        ):
            export.get_details(dynamodb_client=bsm.dynamodb_client)
            if export.is_completed():
                break
            elif export.is_failed():
                raise RuntimeError(f"Dynamodb Export failed!")

    return export


@logger.emoji_block(
    msg="DynamoDB to Sqlite",
    emoji="ðŸ“„",
)
def dynamodb_to_sqlite(
    lang_code: LangCodeEnum,
    export_name: str,
    remove_existing: bool = False,
):
    """
    Load DynamoDB Export data into Sqlite Database. We will use this database
    to track "parse html" job status.
    """
    path_sqlite = dir_missav.joinpath(f"{lang_code.name}.sqlite")
    if remove_existing:  # pragma: no cover
        path_sqlite.remove_if_exists()
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")
    logger.info("Information")
    with logger.indent():
        logger.info(f"{lang_code = }")
        logger.info(f"{export_name = }")
        export = preview_export_details(
            bsm=bsm,
            table_name=klass.Meta.table_name,
            export_name=export_name,
        )
        logger.info(f"path_sqlite = {path_sqlite}")
        logger.info(f"preview path_sqlite at: file://{path_sqlite.parent}")

    engine = sam.engine_creator.EngineCreator.create_sqlite(str(path_sqlite))
    Base.metadata.create_all(engine)

    if export.is_completed() is False:
        raise SystemError(f"Export is not completed yet!")
    data_file_list = export.get_data_files(
        dynamodb_client=bsm.dynamodb_client,
        s3_client=bsm.s3_client,
    )
    job_list = list()
    total = 0
    for data_file in data_file_list:
        s3uri = f"s3://{data_file.s3_bucket}/{data_file.s3_key}"
        logger.info(f"work on data file: {s3uri}")
        s3path = S3Path(s3uri)
        lines = gzip.decompress(s3path.read_bytes()).splitlines()
        counter = 0
        for line in lines:
            item_data = json.loads(line)
            download_task = klass.from_raw_data(item_data["Item"])
            if download_task.is_succeeded():
                job = Job.create_and_not_save(
                    id=download_task.key,
                    url=download_task.url,
                    html=download_task.html,
                )
                job_list.append(job)
                counter += 1
        with logger.indent():
            logger.info(f"got {counter} succeeded items")
        total += counter

    with orm.Session(engine) as ses:
        ses.add_all(job_list)
        ses.commit()

    logger.info(f"got {total} total succeeded items")


@logger.emoji_block(
    msg="Extract Video Details",
    emoji="ðŸ“„",
)
def extract_video_details(
    lang_code: LangCodeEnum,
):
    path_sqlite = dir_missav.joinpath(f"{lang_code.name}.sqlite")
    logger.info("Information")
    with logger.indent():
        logger.info(f"{lang_code = }")

    engine = sam.engine_creator.EngineCreator.create_sqlite(str(path_sqlite))
    for job in Job.query_by_status(
        engine_or_session=engine,
        status=Step2ParseHtmlStatusEnum.pending.value,
        limit=3,
        older_task_first=True,
    ):
        Job.do_parse_html_job(
            engine=engine,
            id=job.id,
            bsm=bsm,
            lang=lang_code,
            debug=True,
        )

    # for job in Job.query_by_status(
    #     engine_or_session=engine,
    #     status=Step2ParseHtmlStatusEnum.succeeded.value,
    #     limit=3,
    #     older_task_first=True,
    # ):
    #     print(job.video_detail)
