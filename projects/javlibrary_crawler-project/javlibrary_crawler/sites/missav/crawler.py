# -*- coding: utf-8 -*-

"""
"""

import typing as T
import os
import time
import json
import math
import gzip
from datetime import datetime, timezone, timedelta

from mpire import WorkerPool
from pathlib_mate import Path
from s3pathlib import S3Path, ContentTypeEnum
import aws_console_url.api as aws_console_url
import aws_arns.api as aws_arns
import sqlalchemy.orm as orm
import sqlalchemy_mate.api as sam

from github import Github

from javlibrary_crawler.utils import (
    get_utc_now,
    to_s3_key_friendly_url,
    preview_export_details,
)
from javlibrary_crawler.logger import logger
from javlibrary_crawler.boto_ses import bsm
from javlibrary_crawler.runtime import runtime
from javlibrary_crawler.config.load import config
from javlibrary_crawler.utils import prompt_to_confirm
from javlibrary_crawler.vendor.dynamodb_export_to_s3 import Export
from javlibrary_crawler.vendor.waiter import Waiter
from javlibrary_crawler.vendor.hashes import hashes, HashAlgoEnum

from .sqlitedb import Base, Step2ParseHtmlStatusEnum, Job
from .constants import (
    LangCodeEnum,
    N_PENDING_SHARD,
    GITHUB_ACTION_RUN_INTERVAL,
    TASK_PROCESSING_TIME,
)
from .paths import dir_missav
from .sitemap import SiteMapSnapshot, parse_item_xml, ItemUrl
from .dynamodb import (
    StatusAndUpdateTimeIndex,
    BaseTask,
    lang_to_step1_mapping,
)
from .downloader import MalformedHtmlError


@logger.emoji_block(
    msg="Create DynamoDB Import Data Files",
    emoji="ğŸ“¥",
)
def create_dynamodb_import_data_files(
    snapshot_id: str,
    lang_code: LangCodeEnum,
    _first_k_file: T.Optional[int] = None,
    _first_k_url: T.Optional[int] = None,
):
    """
    Generate data, format it for DynamoDB import, and upload to S3.

    This function creates the necessary data, structures it in a format
    compatible with DynamoDB import specifications, and uploads the
    resulting file to a designated S3 bucket. The uploaded file can then
    be used for bulk import into DynamoDB.

    è¿™ä¸ªå‡½æ•°ä½¿ç”¨äº†å¤šçº¿ç¨‹, ä¼šå¹¶è¡Œå¤„ç†å¤šä¸ª sitemap_items_*.xml.gz æ–‡ä»¶, å¹¶å°†æ¯ä¸ªæ–‡ä»¶åˆ†åˆ«ä¸Šä¼ åˆ°
    S3. æˆ‘æµ‹è¯•äº†ä¸€ä¸‹, åœ¨ 11 ä¸ªæ ¸å¿ƒçš„ MacBook Pro M3 ä¸Š, å¤„ç† 366 ä¸ª XML æ–‡ä»¶,
    å•çº¿ç¨‹è¦ 214 ç§’, å¤šçº¿ç¨‹è¦ 42 ç§’, é€Ÿåº¦æå‡äº† 5 å€.

    æ³¨, å¦‚æœä½ ä¸æ˜¯ä¸ºäº† debug æˆ–æµ‹è¯•, ä¸è¦ç›´æ¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°. :func:`import_dynamodb_data`
    å‡½æ•°å·²ç»åŒ…å«äº†è¿™ä¸€æ­¥. ç›´æ¥è°ƒç”¨å®ƒæ—¢å¯.
    """

    def func(
        shared_objects: T.Tuple[
            T.Type[BaseTask],
            datetime,
            Path,
        ],
        path: Path,
    ):
        (
            klass,  # DynamoDB ORM å¯¹è±¡
            start_time,  # è¿™ä¸ª start_time ä¼šä½œä¸ºåŸºå‡†ç”¨æ¥ç”Ÿæˆæ¯ä¸ª url çš„ create time
            dir_missav_temp,  # è¿™ä¸ªç›®å½•ç”¨äºå­˜æ”¾ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶
        ) = shared_objects

        logger.info(f"Working on {path.basename} file")
        fname = path.basename.split(".")[0]
        path_temp_json = dir_missav_temp.joinpath(f"{fname}.json")
        path_temp_json_gzip = dir_missav_temp.joinpath(f"{fname}.json.gz")
        logger.info("Write import dynamodb table data to temp json file")
        with logger.indent():
            logger.info(f"preview at: file://{path_temp_json}")

        # ä» xml ä¸­æå– url åˆ—è¡¨
        item_url_list = parse_item_xml(path)
        filtered_item_url_list = ItemUrl.filter_by_lang(
            item_url_list=item_url_list,
            lang_code=lang_code,
        )
        if _first_k_url:  # pragma: no cover
            filtered_item_url_list = filtered_item_url_list[:_first_k_url]
        with logger.indent():
            logger.info(f"Got {len(filtered_item_url_list)} url to insert")

        # æ ¹æ® url ç”Ÿæˆ DynamoDB json å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶
        ith_file = int(path.basename.split("_")[-1].split(".")[0])
        _start_time = start_time + timedelta(seconds=ith_file)
        ith_url = 0
        with path_temp_json.open("a") as f:
            for item in filtered_item_url_list:
                ith_url += 1
                create_time = _start_time + timedelta(microseconds=ith_url)
                task = klass.make(
                    task_id=item.url,
                    create_time=create_time,
                    update_time=create_time,
                )
                f.write(json.dumps({"Item": task.serialize()}) + "\n")
        path_temp_json_gzip.write_bytes(gzip.compress(path_temp_json.read_bytes()))

        # å°†ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ åˆ° S3
        s3dir_temp = config.env.s3dir_missav_dynamodb_import_data.joinpath(
            snapshot_id
        ).to_dir()
        s3path_temp_json_gzip = s3dir_temp.joinpath(f"{fname}.json.gz")
        logger.info("Upload temp json file to S3")
        with logger.indent():
            logger.info(f"preview at: {s3path_temp_json_gzip.console_url}")
        s3path_temp_json_gzip.upload_file(
            path=str(path_temp_json_gzip),
            overwrite=True,
            bsm=bsm,
            extra_args=dict(
                ContentType=ContentTypeEnum.app_gzip,
                Metadata={
                    "sitemap_snapshot_id": snapshot_id,
                    "lang_code": lang_code.name,
                },
            ),
        )

    sitemap_snapshot = SiteMapSnapshot.new(md5=snapshot_id)
    path_list = sitemap_snapshot.get_item_xml_list()
    if _first_k_file:  # pragma: no cover
        path_list = path_list[:_first_k_file]

    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    start_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
    dir_missav_temp = dir_missav.joinpath("temp")
    dir_missav_temp.remove_if_exists()
    dir_missav_temp.mkdir_if_not_exists()

    shared_objects = (klass, start_time, dir_missav_temp)

    st = get_utc_now()

    # --- multi threads mode
    kwargs_list = [dict(path=path) for path in path_list]
    with WorkerPool(
        n_jobs=os.cpu_count(),
        shared_objects=shared_objects,
    ) as pool:
        pool.map(func, kwargs_list)

    # --- single thread mode
    # for path in path_list:
    #     func(shared_objects, path)

    elapse = (get_utc_now() - st).total_seconds()
    logger.info(f"create_dynamodb_import_data_files in {elapse:.2f} seconds.")


@logger.emoji_block(
    msg="Create new DynamoDB by importing data from S3",
    emoji="ğŸ“¥",
)
def import_dynamodb_data(
    snapshot_id: str,
    lang_code: LangCodeEnum,
    _first_k_file: T.Optional[int] = None,
    _first_k_url: T.Optional[int] = None,
):
    """
    **åŠŸèƒ½**

    æˆ‘ä»¬ä¼šå®šæœŸä» sitemap.xml ä¸­ä¸‹è½½ç½‘ç«™ä¸Šæ‰€æœ‰çš„ URL çš„åˆ—è¡¨, ç„¶åå°†å…¶ä½œä¸ºå¾…çˆ¬ URL æ’å…¥åˆ°
    DynamoDB ä¸­. åœ¨ç¬¬ä¸€æ¬¡åˆ›å»º DynamoDB table æ—¶, ç”±äºæ•°æ®é‡å¾ˆå¤§ (å¤§çº¦ 35w+ æ¡æ•°æ®),
    æ‰€ä»¥ç›´æ¥ç”¨ Batch write API å†™å…¥æ•°æ®çš„æ•ˆç‡å¹¶ä¸é«˜.

    ä¸€ä¸ªæ¯”è¾ƒå¥½çš„åšæ³•æ˜¯ç”¨
    `import from S3 <https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/S3DataImport.HowItWorks.html>`_
    åŠŸèƒ½, å…ˆå°†æ•°æ®å†™å…¥ S3, ç„¶åå†ä» S3 ä¸­å¯¼å…¥åˆ° DynamoDB ä¸­. è¿™æ ·åšçš„å¥½å¤„å¾ˆå¤š:

    1. é€Ÿåº¦æ¯”ç›´æ¥ç”¨ Batch write API å¿«å¾ˆå¤š.
    2. å¯ä»¥å¹¶è¡Œæ‰§è¡Œ, ä½¿å¾—é€Ÿåº¦æ›´å¿«.
    3. ç”¨ import å†™å…¥æ•°æ®çš„æ”¶è´¹æ ‡å‡†æ¯” Batch write API ä¾¿å®œ 1/4.

    **æ’å…¥é¡ºåº**

    åœ¨æ’å…¥çš„æ—¶å€™, ä¼˜å…ˆæ’å…¥æ•°å­—å°çš„ sitemap_items_*.xml.gz æ–‡ä»¶ (æ¯”è¾ƒæ—§çš„æ–‡ä»¶). è¿™æ ·èƒ½ä¿è¯
    æ¯”è¾ƒæ–°çš„ url ä¼šæœ‰æ¯”è¾ƒæ–°çš„ update time, è¿™æ ·åœ¨ query çš„æ—¶å€™ç”¨ older_task_first = False
    å¯ä»¥ä¼˜å…ˆç­›é€‰å‡ºæ¯”è¾ƒæ–°çš„ url. ç”±äºæˆ‘ä»¬æ¯æ¬¡æ›´æ–°äº† sitemap.xml ä¹‹åéƒ½ä¼šè·å¾—æ–°çš„ URL, æˆ‘ä»¬
    ä¹Ÿå¸Œæœ›ä¼˜å…ˆä¸‹è½½æ–°çš„ URL, æ‰€ä»¥è¿™ä¸ªæ’å…¥é¡ºåºåˆšå¥½èƒ½æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚.
    """
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"Working on table {klass.Meta.table_name!r}")

    logger.info("WARNING: Import dynamodb table will the delete existing table!")
    prompt_to_confirm()
    if klass.exists():
        klass.delete_table()
        logger.info("wait 15 seconds for table to be deleted ...")
        time.sleep(15)

    with logger.nested():
        create_dynamodb_import_data_files(
            snapshot_id=snapshot_id,
            lang_code=lang_code,
            _first_k_file=_first_k_file,
            _first_k_url=_first_k_url,
        )

    now = get_utc_now()
    client_token = "{}-{}-{}-{}".format(
        str(now.timestamp() // 300),
        snapshot_id,
        _first_k_url,
        _first_k_file,
    )
    s3dir_temp = config.env.s3dir_missav_dynamodb_import_data.joinpath(
        snapshot_id
    ).to_dir()
    res = bsm.dynamodb_client.import_table(
        ClientToken=client_token,
        S3BucketSource=dict(
            S3Bucket=s3dir_temp.bucket,
            S3KeyPrefix=s3dir_temp.key,
        ),
        InputFormat="DYNAMODB_JSON",
        InputCompressionType="GZIP",
        TableCreationParameters=dict(
            TableName=klass.Meta.table_name,
            AttributeDefinitions=[
                dict(AttributeName=BaseTask.key.attr_name, AttributeType="S"),
                dict(AttributeName=BaseTask.value.attr_name, AttributeType="S"),
                dict(AttributeName=BaseTask.update_time.attr_name, AttributeType="S"),
            ],
            KeySchema=[
                dict(AttributeName=BaseTask.key.attr_name, KeyType="HASH"),
            ],
            BillingMode="PAY_PER_REQUEST",
            GlobalSecondaryIndexes=[
                dict(
                    IndexName=StatusAndUpdateTimeIndex.Meta.index_name,
                    KeySchema=[
                        dict(
                            AttributeName=StatusAndUpdateTimeIndex.value.attr_name,
                            KeyType="HASH",
                        ),
                        dict(
                            AttributeName=StatusAndUpdateTimeIndex.update_time.attr_name,
                            KeyType="RANGE",
                        ),
                    ],
                    Projection=dict(
                        ProjectionType="INCLUDE",
                        NonKeyAttributes=[
                            BaseTask.create_time.attr_name,
                        ],
                    ),
                )
            ],
        ),
    )
    import_arn = res["ImportTableDescription"]["ImportArn"]
    with logger.indent():
        logger.info(f"import_arn = {import_arn}")
        logger.info("be patient, it will take a while to import the table.")


@logger.emoji_block(
    msg="Crawl pending tasks",
    emoji="ğŸ•¸",
)
def crawl_pending_tasks(
    lang_code: LangCodeEnum,
):
    """
    **åŠŸèƒ½**

    å» DynamoDB ä¸­æ‰¾åˆ°æœªå®Œæˆçš„ä»»åŠ¡, å¹¶æ‰§è¡Œä¸‹è½½ä»»åŠ¡.

    **Crawler çš„è°ƒåº¦ç­–ç•¥**

    æˆ‘ä»¬åœ¨ ``javlibrary_crawler.sites.missav.constants.GITHUB_ACTION_RUN_INTERVAL`` ä¸­
    å®šä¹‰äº†æ¯ 15 åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ GitHub Action Workflow (ä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åº). æ³¨æ„è¿™é‡Œçš„ 15 åˆ†é’Ÿ
    åªæ˜¯ä¸€ä¸ªä¾‹å­, ç”¨äºè§£é‡Šæˆ‘ä»¬çš„è°ƒåº¦ç­–ç•¥. ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚æ¥è°ƒæ•´è¿™ä¸ªå€¼. åé¢çš„å¾ˆå¤šå…·ä½“æ•°å­—ä¹Ÿæ˜¯ç±»ä¼¼.

    ç„¶åæˆ‘ä»¬å®šä¹‰äº† ``javlibrary_crawler.sites.missav.constants.TASK_PROCESSING_TIME``
    ä¸º 2 ç§’ (æŠ“å– HTML, å†™å…¥ S3, æ›´æ–° DynamoDB åŠ çˆ¬è™«é—´éš”ç­‰å¾… 1 ç§’). äºæ˜¯æˆ‘ä»¬å¯ä»¥ç®—å‡ºæˆ‘ä»¬åœ¨
    900 - 30 (è¿™é‡Œçš„ 30 æ˜¯ä¸ºäº†ä¿é™©èµ·è§æå‰ 30 ç§’ç»“æŸ) = 870 ç§’å†…å¯ä»¥å¤„ç† 870 / 2 = 435 ä¸ªä»»åŠ¡.

    ç”±äºæˆ‘ä»¬çš„ status GSI ä½¿ç”¨äº† sharding ç­–ç•¥, è€Œåœ¨
    ``javlibrary_crawler.sites.missav.constants.N_PENDING_SHARD`` ä¸­å®šä¹‰äº†æˆ‘ä»¬æœ‰ 10 ä¸ª shard.

    æ‰€ä»¥æˆ‘ä»¬åœ¨ query_for_unfinished çš„æ—¶å€™è®¾å®šçš„ LIMIT ç­‰äº 435 / 10 = 43.5, æˆ‘ä»¬å‘ä¸Šå–æ•´, å¾—åˆ° 44.
    è¿™æ ·å¯ä»¥å¤§çº¦æ¯æ¬¡è¿è¡Œ GitHub Action, åœ¨ 870 ç§’å†…éƒ½ä¼šå¤„ç†å®Œ 435 ä¸ªä»»åŠ¡. è¿™æ ·æ—¢é¿å…äº†ä»
    DynamoDB ä¸­è¯»å–è¿‡å¤šçš„æ•°æ®, ä¹Ÿé¿å…äº†ä¸¤ä¸ª Job Run åŒæ—¶è¿è¡Œå¯¼è‡´çš„èµ„æºç«äº‰.
    """
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")

    # Cap the run time of this `crawl_todo` function.
    max_job_run_time = GITHUB_ACTION_RUN_INTERVAL - 30
    LIMIT = math.ceil(max_job_run_time / TASK_PROCESSING_TIME / N_PENDING_SHARD)
    # LIMIT = 1 # for debug only

    # Get list of unfinished (pending and failed) tasks
    task_list: T.List[BaseTask] = klass.query_for_unfinished(
        limit=LIMIT,
        older_task_first=False,
    ).all()
    logger.info(f"Got {len(task_list)} URL to crawl.")

    # figure out GitHub action or local run start time
    if runtime.is_github_action:
        g = Github()
        github_run_id = int(os.environ["GITHUB_RUN_ID"])
        wf_run = g.get_repo("angoraking/javadb-project").get_workflow_run(github_run_id)
        start_at = wf_run.run_started_at
    else:
        start_at = get_utc_now()
    # figure out expected job run end time
    end_at = start_at + timedelta(seconds=max_job_run_time)

    # process each task
    for task in task_list:
        now = get_utc_now()
        logger.info(f"====== Working on {task.url} ======")
        logger.info(f"now is {now}, this job will end at {end_at}")
        # if we are running out of time, stop the job
        if now >= end_at:
            logger.info("Time is up!")
            break

        # if we don't have enough time for the next job, stop this job run
        how_many_time_left = (end_at - now).total_seconds()
        if how_many_time_left < TASK_PROCESSING_TIME:
            logger.info("Time is up!")
            break

        try:
            with klass.start(
                task_id=task.task_id,
                debug=True,
            ) as exec_ctx:
                task_on_the_fly: BaseTask = exec_ctx.task
                task_on_the_fly.do_download_task()  # this function has auto retry
        # we don't want to stop the job run because of MalformedHtmlError
        # (mostly ServerSide error)
        except MalformedHtmlError as e:
            pass
        except Exception as e:
            raise e

        time.sleep(1)


def export_dynamodb(
    lang_code: LangCodeEnum,
    export_time: T.Optional[datetime] = None,
    delays: int = 10,
    timeout: int = 600,
    wait: bool = True,
) -> Export:
    """
    å°† DynamoDB æ•´ä¸ªè¡¨å¯¼å‡ºåˆ° S3 ä¸­.
    """
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")
    url = klass.get_table_overview_console_url()
    print(f"preview table: {url}")
    res = bsm.dynamodb_client.update_continuous_backups(
        TableName=klass.Meta.table_name,
        PointInTimeRecoverySpecification={"PointInTimeRecoveryEnabled": True},
    )

    if export_time is None:
        export_time = get_utc_now()
    client_token = export_time.strftime("%Y-%m-%d %H:%M:%S.%f")
    print(f"{client_token = }")
    table_arn = aws_arns.res.DynamodbTable.new(
        aws_account_id=bsm.aws_account_id,
        aws_region=bsm.aws_region,
        table_name=klass.Meta.table_name,
    ).to_arn()
    print(f"export s3 location = {config.env.s3dir_missav_dynamodb_exports.uri}")
    print(
        f"preview export s3 location = {config.env.s3dir_missav_dynamodb_exports.console_url}"
    )

    export = Export.export_table_to_point_in_time(
        dynamodb_client=bsm.dynamodb_client,
        table_arn=table_arn,
        s3_bucket=config.env.s3dir_missav_dynamodb_exports.bucket,
        s3_prefix=config.env.s3dir_missav_dynamodb_exports.key,
        export_time=export_time,
        client_token=client_token,
    )
    export_name = export.arn.split("/")[-1]
    aws_console = aws_console_url.AWSConsole.from_bsm(bsm)
    export_url = aws_console.dynamodb.get_table_export(
        table_or_arn=table_arn,
        export_name=export_name,
    )
    print(f"{export_url = }")

    if wait:
        for _ in Waiter(
            delays=delays,
            timeout=timeout,
        ):
            export.get_details(dynamodb_client=bsm.dynamodb_client)
            if export.is_completed():
                break
            elif export.is_failed():
                raise RuntimeError(f"Dynamodb Export failed!")

    return export


@logger.emoji_block(
    msg="DynamoDB to Sqlite",
    emoji="ğŸ“„",
)
def dynamodb_to_sqlite(
    lang_code: LangCodeEnum,
    export_name: str,
    remove_existing: bool = False,
):
    """
    Load DynamoDB Export data into Sqlite Database. We will use this database
    to track "parse html" job status.
    """
    path_sqlite = dir_missav.joinpath(f"{lang_code.name}.sqlite")
    if remove_existing:  # pragma: no cover
        path_sqlite.remove_if_exists()
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")
    logger.info("Information")
    with logger.indent():
        logger.info(f"{lang_code = }")
        logger.info(f"{export_name = }")
        export = preview_export_details(
            bsm=bsm,
            table_name=klass.Meta.table_name,
            export_name=export_name,
        )
        logger.info(f"path_sqlite = {path_sqlite}")
        logger.info(f"preview path_sqlite at: file://{path_sqlite.parent}")

    engine = sam.engine_creator.EngineCreator.create_sqlite(str(path_sqlite))
    Base.metadata.create_all(engine)

    if export.is_completed() is False:
        raise SystemError(f"Export is not completed yet!")
    data_file_list = export.get_data_files(
        dynamodb_client=bsm.dynamodb_client,
        s3_client=bsm.s3_client,
    )
    job_list = list()
    total = 0
    for data_file in data_file_list:
        s3uri = f"s3://{data_file.s3_bucket}/{data_file.s3_key}"
        logger.info(f"work on data file: {s3uri}")
        s3path = S3Path(s3uri)
        lines = gzip.decompress(s3path.read_bytes()).splitlines()
        counter = 0
        for line in lines:
            item_data = json.loads(line)
            download_task = klass.from_raw_data(item_data["Item"])
            if download_task.is_succeeded():
                job = Job.create_and_not_save(
                    id=download_task.key,
                    url=download_task.url,
                    html=download_task.html,
                )
                job_list.append(job)
                counter += 1
        with logger.indent():
            logger.info(f"got {counter} succeeded items")
        total += counter

    with orm.Session(engine) as ses:
        ses.add_all(job_list)
        ses.commit()

    logger.info(f"got {total} total succeeded items")


@logger.emoji_block(
    msg="Extract Video Details",
    emoji="ğŸ“„",
)
def extract_video_details(
    lang_code: LangCodeEnum,
):
    path_sqlite = dir_missav.joinpath(f"{lang_code.name}.sqlite")
    logger.info("Information")
    with logger.indent():
        logger.info(f"{lang_code = }")

    engine = sam.engine_creator.EngineCreator.create_sqlite(str(path_sqlite))
    for job in Job.query_by_status(
        engine_or_session=engine,
        status=Step2ParseHtmlStatusEnum.pending.value,
        limit=3,
        older_task_first=True,
    ):
        Job.do_parse_html_job(
            engine=engine,
            id=job.id,
            bsm=bsm,
            lang=lang_code,
            debug=True,
        )

    # for job in Job.query_by_status(
    #     engine_or_session=engine,
    #     status=Step2ParseHtmlStatusEnum.succeeded.value,
    #     limit=3,
    #     older_task_first=True,
    # ):
    #     print(job.video_detail)


@logger.emoji_block(
    msg="Insert pending tasks to DynamoDB",
    emoji="ğŸ“¥",
)
def insert_pending_tasks(
    snapshot_id: str,
    lang_code: LangCodeEnum,
    export_arn: str,
):
    """
    **åŠŸèƒ½**

    æˆ‘ä»¬ä¼šå®šæœŸä»æœ€æ–°çš„ sitemap.xml ä¸­ä¸‹è½½ç½‘ç«™ä¸Šæ‰€æœ‰çš„ URL çš„åˆ—è¡¨, ç„¶åå°† DynamoDB ä¸­
    æ²¡æœ‰çš„ URL éƒ½ä½œä¸º pending task å†™å…¥åˆ° DynamoDB ä¸­.

    **æ’å…¥é¡ºåº**

    åœ¨æ’å…¥çš„æ—¶å€™, ä¼˜å…ˆæ’å…¥æ•°å­—å°çš„ sitemap_items_*.xml.gz æ–‡ä»¶ (æ¯”è¾ƒæ—§çš„æ–‡ä»¶). è¿™æ ·èƒ½ä¿è¯
    æ¯”è¾ƒæ–°çš„ url ä¼šæœ‰æ¯”è¾ƒæ–°çš„ update time, è¿™æ ·åœ¨ query çš„æ—¶å€™ç”¨ older_task_first = False
    å¯ä»¥ä¼˜å…ˆç­›é€‰å‡ºæ¯”è¾ƒæ–°çš„ url. ç”±äºæˆ‘ä»¬æ¯æ¬¡æ›´æ–°äº† sitemap.xml ä¹‹åéƒ½ä¼šè·å¾—æ–°çš„ URL, æˆ‘ä»¬
    ä¹Ÿå¸Œæœ›ä¼˜å…ˆä¸‹è½½æ–°çš„ URL, æ‰€ä»¥è¿™ä¸ªæ’å…¥é¡ºåºåˆšå¥½èƒ½æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚.

    **Sitemap æ›´æ–°ç­–ç•¥**

    ç”±äºæ’å…¥æ–° URL æ˜¯ä¸€ä¸ªæ¯”è¾ƒ

    :param snapshot_id: sitemap çš„ MD5 å“ˆå¸Œå€¼
    :param lang_code: è¯­è¨€ä»£ç , è¿™ä¼šå†³å®šæ•°æ®ä¼šæ’å…¥åˆ°å“ªä¸ªè¡¨ä¸­
    :param export_arn: å¦‚æœä½ çš„ DynamoDB ä¸­å·²ç»æœ‰å¾ˆå¤šæ•°æ®äº†, ç”±äºç”¨ DynamoDB ç›´æ¥
        filter å“ªäº› url å·²ç»å­˜åœ¨å“ªäº›ä¸å­˜åœ¨ä¸æ˜¯å¾ˆæ–¹ä¾¿, æ‰€ä»¥ä¼šæå‰å°†æ•°æ®å¯¼å‡ºåˆ° S3 ä¸­.
    """
    sitemap_snapshot = SiteMapSnapshot.new(md5=snapshot_id)
    klass: T.Type[BaseTask] = lang_to_step1_mapping[lang_code.value]
    klass.set_connection(bsm)
    logger.info(f"working on table {klass.Meta.table_name!r}")
    with klass.batch_write() as batch:
        # filter by only
        if export_arn:
            raise NotImplementedError
        else:
            path_list = sitemap_snapshot.get_item_xml_list()
            path_list = path_list[:1]
            logger.info(f"Got {len(path_list)} xml file to insert")
            for path in path_list:
                logger.info(f"Working on {path.basename} file")
                item_url_list = parse_item_xml(path)
                filtered_item_url_list = ItemUrl.filter_by_lang(
                    item_url_list, lang_code
                )
                filtered_item_url_list = filtered_item_url_list[:1000]
                with logger.indent():
                    logger.info(f"Got {len(filtered_item_url_list)} url to insert")
                for item in filtered_item_url_list:
                    task = klass.make_and_save(task_id=item.url)
                    batch.save(task)
