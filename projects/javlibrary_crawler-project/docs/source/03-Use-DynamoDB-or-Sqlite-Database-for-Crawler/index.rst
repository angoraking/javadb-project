.. _use-dynamodb-or-sqlite-database-for-crawler:

Use DynamoDB or Sqlite Database for Crawler
==============================================================================


Overview
------------------------------------------------------------------------------
爬虫的任务调度本质是把很多 URL 所对应的 HTML 抓取下来, 并从中提取数据. 这一般需要用到数据库来记录每个 URL 是否被成功抓取了. 但是我们的爬虫程序并不是 24 小时不间断运行的, 但数据库一般需要 24 小时不断运行, 这就会导致无法充分使用云托管的数据库. 而我也考虑过 DynamoDB, 但是对于这个高频的读写需求, 我估计最终的价格也不会低, 所以我决定尝试使用 sqlite 作为我的爬虫数据库.


Traffic
------------------------------------------------------------------------------
我按照每 5 秒访问一次目标网站, 抓取一个 URL. 每处理一个 URL 需要 1 次读 + 一次写来获取锁, 然后用一次写来更新状态. 我以 missav 网站为例, 一共有大约 4.5M 个 URL 需要抓取. 总耗时大约 4500000 * 5 / 3600 = 6,250 小时 = 260 天. 每个 URL 的 status tracking 的数据大约在 0.25KB 左右, 成功 parse 出数据后大约在 2KB 左右. 所以总共的数据量大约是 2KB * 4.5M 等于 9GB.


RDS Pricing
------------------------------------------------------------------------------
这种程度的 workload 最小的 `db.t4g.micro <https://aws.amazon.com/rds/instance-types/>`_ 就够用了, 如果使用 single az 的部署方案 (我们不需要高可用), `每小时是 0.016 <https://aws.amazon.com/rds/postgresql/pricing/?pg=pr&loc=3>`_, 一个月就是 0.016 * 24 * 30 = $11.52. 如果我们要对数据做 Scan 之类的分析或批处理, 我们可以将数据库 Snapshot 导出到 S3 中然后进行分析. 我们整个数据库的大小大约是 9GB 左右, 按照官方给出的费率 $0.01/GB 的价格, 导出一共是 $0.09.

由于在运行爬虫期间数据库需要一直在线, 所以我们为了抓取全部数据需要让数据库运行 260 天左右, 也就是 9 个月. 为了抓取全部数据我们需要花 11.52 * 9 = **$100**.


DynamoDB Pricing
------------------------------------------------------------------------------
DynamoDB 有 On-Demand 和 Provisioned 两种, 我们使用 On-Demand 模式. 在这种模式下, 一个 WRU 是 1KB 写入量, 一个 RRU (eventual consistency) 是 8KB 的读取量. 我们的 DynamoDB item 大小平均是没有 parsed 数据的情况下是 0.1 KB, 有数据后是 2KB. 所以每个 URL 要消耗 1 WRU 用来初次创建 item, 1 个 RRU + 1 个 WRU 来获取锁, 1 个 WRU 来更新数据. 所以一共是 1 RRU + 3 WRU. 这样算抓完全部 4.5M 个 URL 需要 4.5M RRU + 13.5M WRU, 按照官方给出的费率 $0.25 per million RRU, $1.25 per WRU 来算, 大约全部抓完是 4.5 * 0.25 + 13.5 * 1.25 = $18. 考虑到 DynamoDB 是 serverless 的, 不发生读写就不会收费, 所以 $18 就能抓完整个数据集还是很划算的.

如果我们要对整个数据库做 Scan 之类的分析或批处理, 我们可以将数据库导出到 S3 中然后进行分析. 我们整个数据库的大小大约是 9GB 左右, 按照官方给出的费率 0.10/GB 的价格, 导出一共是 $1. 注意, DynamoDB 在 2023 年 9 月退出了一项新功能, 可以只将增量数据导出到 S3. 不管数据库最后变多大, 我们为了获取最新数据的 snapshot 所需要导出的数据量永远跟总数据量相当.

为了抓取全部数据我们需要花 18 + 1 + 1 = **$20**.

Reference:

- DynamoDB On-Demand Pricing: https://aws.amazon.com/dynamodb/pricing/on-demand/


Sqlite Pricing
------------------------------------------------------------------------------
这种情况下我们实际是将 sqlite 保存到 S3 上, 每次要用的时候下载下来, 用完了上传回去. 为了避免因为程序错误而丢失之前抓取的数据, 我们每 5 分钟进行一次下载上传, 按照 5 秒抓取一次, 5 分钟可以抓取 60 条记录, 而抓完 4.5M 个 URL 需要 4.5M / 60 = 75,000 次 Sqlite 的上传下载.

由于 sqlite 在保存数据时候有优化, 所以实际消耗的磁盘要比你把每条记录当成 JSON 算要小, 经过我统计大约是一半左右. 所以最终在 sqlite 中的数据量大约是 4.5GB. 为了避免每次上传下载都需要下载全部数据库整个 4.5GB 数据库 (最终 Sqlite 中的数据会达到 4.5GB), 我们会将数据库流量用哈希算法打散, 例如分拆成 1000 个小文件, 每个文件是 4.5MB 左右. 由于整个数据是线性增长的, 所以平均下载一次的流量我们按照 4.5MB 的一半 2.25MB 算.

- Storage: 费率 0.023 GB/Month, 总费用 9GB * 0.023 = $0.207/Month
- Data Transfer: 由于上传不花钱, 下载花钱, 按照官方给出的 $0.09/GB 的费率, 下载 sqlite 数据库一共需要花 75000 * 2.25 * 0.09 / 1000 = $15.1875.

为了抓取全部数据我们需要花 18 + 1 + 1 = **$20**.


Comparison
------------------------------------------------------------------------------
- SQL: 直接不用考虑了, 数据库需要一直在线有点伤不起.
- DynamoDB: 非常值得考虑, 减少了大量自己维护数据库的麻烦. 就是进行 Scan 和导出的时候麻烦点.
- Sqlite: 也很值得考虑, 自己的代码量会多很多, 但是 Scan 和查询的时候就比较容易了.

我目前比较倾向于使用 DynamoDB. 把 S3 当 Sqlite 用在数据量大的时候, 抓取较为频繁的时候开销会比较大. 并且如果想要减少开销就需要让每次上传下载的间隔长一些, 这样做会导致数据丢失的风险变大. 所以综合考虑还是用 DynamoDB 比较靠谱.
